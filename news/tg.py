import json
import re
import time
from datetime import datetime

from bs4 import BeautifulSoup

import feedparser
import requests
from environs import Env


def clean_html(html):
    soup = BeautifulSoup(html, 'html.parser')

    # æ›¿æ¢æ‰€æœ‰ <br> æ ‡ç­¾ä¸ºæ¢è¡Œç¬¦
    for br in soup.find_all('br'):
        br.replace_with('\n')

    # ç§»é™¤æ‰€æœ‰æ ‡ç­¾ï¼ŒåŒ…æ‹¬å›¾ç‰‡æ ‡ç­¾
    for tag in soup.find_all(True):
        tag.decompose()

    # æå–æ–‡æœ¬å†…å®¹
    text_content = soup.get_text()

    return text_content


def remove_footer(text):
    text = re.sub(r'[â˜˜ï¸ğŸ“®]+', '', text)
    text = re.sub(r'å…³æ³¨é¢‘é“\s@(\w+)', '', text)
    text = re.sub(r'æŠ•ç¨¿çˆ†æ–™\s@(\w+)', '', text)

    return text.strip()


def get_rss(token):
    url = 'https://api.xbxin.com/kv'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {token}'
    }

    body = {
        'action': 'read',
        'key': 'rss'
    }

    response = requests.post(url, headers=headers, json=body)
    if response.status_code == 200:
        data = response.json()
        rss = data['data']['value']
        return json.loads(rss)
    elif response.status_code == 404:
        print('Key not found')
        return None
    else:
        return None


def parse(url, history):
    # è§£æRSS
    feed = feedparser.parse(url)

    history = datetime.fromisoformat(history)
    new_date = history

    for entry in feed.entries:
        pub_date_str = entry.published
        pub_date = datetime.fromisoformat(pub_date_str)
        if pub_date > new_date:
            new_date = pub_date
        if pub_date <= history:
            break

        title = entry.title
        end_index = title.find('ã€‚')
        if end_index == -1:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥å·ï¼Œåˆ™æ‰¾ç¬¬ä¸€ä¸ªé€—å·
            end_index = title.find('ï¼Œ')
        if end_index != -1:
            title = title[:end_index + 1]  # åŒ…æ‹¬å¥å·æˆ–é€—å·

        desc = entry.description
        # ä½¿ç”¨ BeautifulSoup å¤„ç†å’Œæ¸…ç† HTML æ ‡ç­¾
        clean_desc = clean_html(desc)

        # æ„é€ æ¶ˆæ¯å†…å®¹
        message = f'*{title}*\n{clean_desc}'
        message = remove_footer(message)

        send_to(message)

    return new_date.isoformat()


def send_to(message):
    # è¯·æ±‚æ•°æ®
    data = {
        "action": "tg",
        'to': 'houinin',
        'message': message,
        'msg_type': 'Markdown'
    }

    api_url = 'https://api.xbxin.com/msg'

    # å‘é€è¯·æ±‚
    response = requests.post(api_url, json=data)

    # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
    if response.status_code == 200:
        print(f'æ¶ˆæ¯æ¨é€æˆåŠŸ')
    else:
        print(f'æ¶ˆæ¯æ¨é€å¤±è´¥: {response.text}')


def save_rss(token, rss):
    url = 'https://api.xbxin.com/kv'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {token}'
    }

    body = {
        'action': 'write',
        'key': 'rss',
        'value': json.dumps(rss)
    }

    response = requests.post(url, headers=headers, json=body)
    if response.status_code == 200:
        print('RSSä¿å­˜æˆåŠŸ')
    else:
        print(f'RSSä¿å­˜å¤±è´¥: {response.text}')


def main():
    env = Env()
    env.read_env()
    token = env.str('TOKEN')

    Route = {
        'tg': '/telegram/channel',
        'weibo': '/weibo/user',
        'xhs': '/xiaohongshu/user'
    }
    baseurl = 'https://rss.xbxin.com/rss'

    rss = get_rss(token)

    for item in rss:
        for sub in rss[item]:
            rss_url = f'{baseurl}{Route[item]}/{sub}'
            new_date = parse(rss_url, rss[item][sub])
            rss[item][sub] = new_date
            time.sleep(5)

    save_rss(token,rss)


if __name__ == '__main__':
    main()
